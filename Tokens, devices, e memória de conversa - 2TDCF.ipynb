{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMYvqwdA7eoQA2ynyeK4wEb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qN2LjUTPYbqJ"},"outputs":[],"source":["# Entendendo a geração de próximos tokens, diferenças devices, e lembrando o que foi dito"]},{"cell_type":"code","source":["# vendo se temos GPU disponível\n","\n","# !nvidia-smi <-- não vai funcionar se não tiver GPU!\n","\n","import time\n","import numpy as np\n","import torch\n","import transformers\n","\n","print(torch.cuda.is_available()) # retorna True se encontrou uma GPU para mandarmos jobs\n","\n","# dinamicamente identificando o dispositivo\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bczkcGBwZWvp","executionInfo":{"status":"ok","timestamp":1725547372297,"user_tz":180,"elapsed":4885,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"e2d67de0-0cc0-4b7b-dd3b-e7f1ce6e7810"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","cpu\n"]}]},{"cell_type":"code","source":["tiny_llama = transformers.pipeline(\n","    task='text-generation',\n","    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n","    device=device\n",")\n","\n","print(tiny_llama)\n","print(tiny_llama.model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKn4EABQaU57","executionInfo":{"status":"ok","timestamp":1725547402946,"user_tz":180,"elapsed":30659,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"06b080c0-59db-4365-e5ec-62b198e2999e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f2b823cae30>\n","LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n"]}]},{"cell_type":"code","source":["prompt = \"Today is a beautiful day to\"\n","\n","# Maneira mais direta possivel de usar um modelo. Ele vai usar os parametros padroes para gerar a saida\n","output = tiny_llama(prompt, max_new_tokens=25)\n","\n","# Se nao especificarmos o maximo de novos tokens, ele vai gerar ate encontrar um token que sinaliza\n","# fim da sentenca\n","\n","print(output)\n","print(output[0]['generated_text']) # --> vamos tacar aqui https://platform.openai.com/tokenizer e ver como o GPT faria a tokenizacao\n","# Ele encontrou o mesmo numero de tokens que limitamos a geracao"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IHSjGO9bRec","executionInfo":{"status":"ok","timestamp":1725547414896,"user_tz":180,"elapsed":11958,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"61bbf28a-c169-487a-8053-36399465cbab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': 'Today is a beautiful day to be alive.\\n\\nVerse 2:\\nThe sun is shining bright, the birds are singing,\\nThe'}]\n","Today is a beautiful day to be alive.\n","\n","Verse 2:\n","The sun is shining bright, the birds are singing,\n","The\n"]}]},{"cell_type":"code","source":["# vendo token a token o processo de geracao\n","\n","# tensor eh o array que redes neurais utilizam. pt --> pytorch. tf--> tensorflow.\n","# soh o tensor tem o metodo to(device), que serve para mover o array para o mesmo\n","# dispositivo que o nosso modelo esta\n","prompt_input_ids = tiny_llama.tokenizer.encode(prompt, return_tensors='pt').to(device)\n","print(prompt_input_ids)\n","\n","# para INSPECIONAR a geracao de tokens, precisamos gerar o output com novos parametros\n","output = tiny_llama.model.generate(\n","    prompt_input_ids, max_new_tokens=25,\n","    return_dict_in_generate=True, # retorne um dicionario com informacoes da geracao de dados\n","    output_scores=True, # retorne as pontuacoes de cada token gerado\n",")\n","\n","print(output.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8o6hTW8XctIk","executionInfo":{"status":"ok","timestamp":1725547427063,"user_tz":180,"elapsed":12217,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"42d8e54c-22e9-465e-e022-14ff7a5b13cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    1, 20628,   338,   263,  9560,  2462,   304]])\n","odict_keys(['sequences', 'scores', 'past_key_values'])\n"]}]},{"cell_type":"code","source":["# vamos pegar os TRANSITION SCORES (probabilidades da palavra utilizada)\n","\n","transitions = tiny_llama.model.compute_transition_scores(\n","    output.sequences, output.scores, normalize_logits=True\n",")\n","\n","transitions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7T3U0SYweRE-","executionInfo":{"status":"ok","timestamp":1725547427064,"user_tz":180,"elapsed":26,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"04bc7368-b5fb-4df2-a04b-0b883b1defe5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.5247e+00, -5.8344e-01, -9.5884e-01, -1.5707e+00, -1.4555e+00,\n","         -1.5399e+00, -9.6954e-04, -2.2908e-02, -1.0781e+00, -6.2057e-02,\n","         -1.6606e-01, -2.1464e+00, -6.8957e-01, -9.2513e-01, -2.5262e-01,\n","         -4.2558e-03, -7.1952e-01, -2.9928e-01, -1.0641e+00, -1.2072e+00,\n","         -3.4627e-02, -1.2238e-01, -4.7702e-01, -1.8247e-01, -5.4553e-01]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# soh quero ver a sequencia de transicao do que foi gerado (ignora o prompt original)\n","\n","tamanho_prompt = len(prompt_input_ids[0])\n","print(tamanho_prompt)\n","\n","# nosso prompt faz parte da resposta gerada!\n","print(output.sequences[0][:tamanho_prompt])\n","\n","# Pegando soh o que foi gerado\n","generated_tokens = output.sequences[0][tamanho_prompt:]\n","print(generated_tokens)\n","\n","print('| token id | score  | token str | prob % |')\n","print('|----------+--------+-----------+--------|')\n","for (token, score) in zip(generated_tokens, transitions[0]):\n","    if tiny_llama.tokenizer.decode(token) == '\\n':\n","      continue\n","\n","    print(f\"| {token:8d} | {score.to('cpu').numpy():.3f} | {tiny_llama.tokenizer.decode(token):9s} | {np.exp(score.to('cpu').numpy()):.4f} |\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9W4-uzUke9me","executionInfo":{"status":"ok","timestamp":1725547488869,"user_tz":180,"elapsed":341,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"2857d433-341a-4eb4-ae9d-45ecdb1b1eed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7\n","tensor([    1, 20628,   338,   263,  9560,  2462,   304])\n","tensor([  367, 18758, 29889,    13,    13,  6565,   344, 29871, 29906, 29901,\n","           13,  1576,  6575,   338,   528,  2827, 11785, 29892,   278, 17952,\n","          526, 23623, 29892,    13,  1576])\n","| token id | score  | token str | prob % |\n","|----------+--------+-----------+--------|\n","|      367 | -1.525 | be        | 0.2177 |\n","|    18758 | -0.583 | alive     | 0.5580 |\n","|    29889 | -0.959 | .         | 0.3833 |\n","|     6565 | -1.540 | Ver       | 0.2144 |\n","|      344 | -0.001 | se        | 0.9990 |\n","|    29871 | -0.023 |           | 0.9774 |\n","|    29906 | -1.078 | 2         | 0.3402 |\n","|    29901 | -0.062 | :         | 0.9398 |\n","|     1576 | -2.146 | The       | 0.1169 |\n","|     6575 | -0.690 | sun       | 0.5018 |\n","|      338 | -0.925 | is        | 0.3965 |\n","|      528 | -0.253 | sh        | 0.7768 |\n","|     2827 | -0.004 | ining     | 0.9958 |\n","|    11785 | -0.720 | bright    | 0.4870 |\n","|    29892 | -0.299 | ,         | 0.7413 |\n","|      278 | -1.064 | the       | 0.3450 |\n","|    17952 | -1.207 | birds     | 0.2990 |\n","|      526 | -0.035 | are       | 0.9660 |\n","|    23623 | -0.122 | singing   | 0.8848 |\n","|    29892 | -0.477 | ,         | 0.6206 |\n","|     1576 | -0.546 | The       | 0.5795 |\n"]}]},{"cell_type":"code","source":["# Vendo o tempo de execucao para gerar um prompt\n","\n","start = time.time()\n","\n","output = tiny_llama(prompt, max_new_tokens=100)\n","\n","end = time.time()\n","\n","print(tiny_llama.model)\n","print(f'Tempo de execucao: {end - start:.2f} segundos')\n","print(output)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X026yUBZkJar","executionInfo":{"status":"ok","timestamp":1725547545352,"user_tz":180,"elapsed":51403,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"df640729-12d4-4979-e32e-fe53f34d0c3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n","Tempo de execucao: 51.05 segundos\n","[{'generated_text': \"Today is a beautiful day to be alive.\\n\\nVerse 2:\\nThe sun is shining bright, the birds are singing,\\nThe world is full of love and joy,\\nLet's embrace this moment, let's be free,\\nLet's live life to the fullest, let's be happy.\\n\\nChorus:\\nToday, today, today, today, today, today, today, today, today, today, today, today, today, today\"}]\n","Today is a beautiful day to be alive.\n","\n","Verse 2:\n","The sun is shining bright, the birds are singing,\n","The world is full of love and joy,\n","Let's embrace this moment, let's be free,\n","Let's live life to the fullest, let's be happy.\n","\n","Chorus:\n","Today, today, today, today, today, today, today, today, today, today, today, today, today, today\n"]}]},{"cell_type":"code","source":["# Lembrando da conversa\n","\n","prompt1 = \"What day is today?\"\n","prompt2 = \"What day is tomorrow?\"\n","\n","output = tiny_llama(prompt1, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","\n","print(\"-\"*80)\n","\n","output = tiny_llama(prompt2, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","\n","# Ué, o que aconteceu? Alucinou?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSX7iqUapcWX","executionInfo":{"status":"ok","timestamp":1725548024007,"user_tz":180,"elapsed":13905,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"8e016ad1-583d-4b68-fe62-ee3d0fcc67cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["What day is today?\n","\n","Student: It's Monday.\n","\n","--------------------------------------------------------------------------------\n","What day is tomorrow?\n","\n","Tomorrow is Monday.\n","\n","Inter\n"]}]},{"cell_type":"code","source":["# para se lembrar, precisamos alimentar ele com TUDO QUE FOI GERADO ANTERIORMENTE\n","\n","prompt1 = \"What day is today?\"\n","prompt2 = \"What day is tomorrow?\"\n","\n","output = tiny_llama(prompt1, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","\n","print(\"-\"*80)\n","\n","output = tiny_llama(output[0]['generated_text'] + prompt2, max_new_tokens=30)\n","print(output[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yKENU4Zqw03","executionInfo":{"status":"ok","timestamp":1725548010138,"user_tz":180,"elapsed":25444,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"}},"outputId":"3d624747-848c-4be7-d97c-7c2d807ace9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["What day is today?\n","\n","Student: It's Monday.\n","\n","--------------------------------------------------------------------------------\n","What day is today?\n","\n","Student: It's Monday.\n",". What day is tomorrow?\n","\n","Student: It's Tuesday.\n",". What day is next week?\n","\n","Student: It's Wednesday.\n"]}]},{"cell_type":"code","source":["# vamos carregar outro modelo (alucinou demais anteriormente)\n","\n","# O código abaixo vai explodir a memória ram (pq ja temos o outro modelo carregado nela).\n","# deletando e limpando a memória\n","\n","# import gc # garbage colector\n","# del tiny_llama\n","# gc.collect()\n","# torch.cuda.empty_cache()\n","\n","# falcon = transformers.pipeline(\n","#     task='text-generation',\n","#     model='tiiuae/falcon-7b',\n","#     device=device\n","# )"],"metadata":{"id":"Ju7FyKeGjSo8"},"execution_count":null,"outputs":[]}]}